# KoGPT2

- KoGPT2?
    - GPT-2ì˜ ë¶€ì¡±í•œ í•œêµ­ì–´ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´, 40GB ì´ìƒì˜ í•œêµ­ì–´ corpusë¡œ í•™ìŠµëœ ëª¨ë¸!
    - [https://github.com/SKT-AI/KoGPT2](https://github.com/SKT-AI/KoGPT2)
    - ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” `skt/kogpt2-base-v2` ëª¨ë¸ ì‚¬ìš©

- Tokenizer
    - í—ˆê¹…í˜ì´ìŠ¤ [tokenizers](https://github.com/huggingface/tokenizers) íŒ¨í‚¤ì§€ì˜ Character BPE tokenizer ì‚¬ìš©í–ˆë‹¤ê³  í•©ë‹ˆë‹¤.
    - vocab sizeëŠ” 51,200ì´ê³ , ì¼ë¶€ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì´ëª¨ì§€ì™€ ì´ëª¨í‹°ì½˜ë„ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.
    - ê·¸ ì™¸ì— `unused0` ~ `unused99` ì˜ ë¯¸ì‚¬ìš© í† í°ì´ ì¡´ì¬í•˜ëŠ”ë°, ì´ëŠ” ê°ì ì •ì˜í•˜ì—¬ taskì— ë§ì¶° ì‚¬ìš©í•˜ë©´ ëœë‹¤ê³  í•©ë‹ˆë‹¤.
    
    ```python
    > from transformers import PreTrainedTokenizerFast
    > tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
      bos_token='</s>', eos_token='</s>', unk_token='<unk>',
      pad_token='<pad>', mask_token='<mask>')
    > tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")
    ['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']
    ```

- Data
  - KoGPT2ì˜ ê²½ìš° `í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼`, ë‰´ìŠ¤, `ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0`, `ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì›` ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©
  - ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì—¬ëŸ¬ `ë™í™” ë°ì´í„°`ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì‹œì¼œ ë™í™” ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
    - ì–´ë¦°ì´ ì²­ì™€ëŒ€ - ì „ë˜ë™í™” 100ì„ 
    - ê·¸ë¦¼í˜•ì œ ë™í™” ëª¨ìŒì§‘
    - êµ­ë¦½ êµ­ì–´ì› ë¹„ì¶œíŒë¬¼ ë°ì´í„°
    - ê·¸ ì™¸ì—ë„ ê³„ì† ì“¸ë§Œí•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì°¾ì•„ë³´ê³  ìˆìŠµë‹ˆë‹¤.

- Usage

```bash
python kogpt2_trainer.py # ëª¨ë¸ í•™ìŠµ ë° output í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
python kogpt2_inference.py # í•™ìŠµí•œ ëª¨ë¸ì„ ì´ìš©í•´ì„œ í…ŒìŠ¤íŠ¸ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```
> inferenceì‹œ ì—¬ëŸ¬ parameterë¥¼ ì¡°ì •í•˜ì—¬ ê²°ê³¼ë¬¼ì„ ë‹¤ì–‘í•˜ê²Œ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ì„ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.